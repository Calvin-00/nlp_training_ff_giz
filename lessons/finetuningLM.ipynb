{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install transformers datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Finetuning Language models for different downstream tasks\n",
    " The following are the steps for text classification\n",
    "* Load the dataset\n",
    "* Choose model to use\n",
    "* Preprocess the data\n",
    "* Initiate the modelâ€™s tokenizer\n",
    "* Tokenize the preprocessed dataset\n",
    "* Turn the dataset into a Dataset object\n",
    "* Train the model\n",
    "    * Using API \n",
    "    * Using pytorch native code\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def loaddataset(filepath):\n",
    "    ''' filepath is the csv path of the dataset\n",
    "    The output dataset is a dataframe with column sentence and label\n",
    "    '''\n",
    "    dataset = pd.DataFrame()\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO Split the training dataset into training and validation. tip use train_test_split() from sklearn\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO load LM tokenizer\n",
    "#Example\n",
    "# from transformers import DistilBertTokenizerFast\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO tokenize the training, validation and testing text\n",
    "#Example \n",
    "# inputs = tokenizer(sentences, padding=\"max_length\", truncation=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "class DatasetObject(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "#TODO - turn training, validdation and testing labels and encodings to dataset object"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO train the model - using API\n",
    "# from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          # output directory\n",
    "#     num_train_epochs=3,              # total number of training epochs\n",
    "#     per_device_train_batch_size=16,  # batch size per device during training\n",
    "#     per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='./logs',            # directory for storing logs\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "#     args=training_args,                  # training arguments, defined above\n",
    "#     train_dataset=train_dataset,         # training dataset\n",
    "#     eval_dataset=val_dataset             # evaluation dataset\n",
    "#     compute_metrics=compute_metrics      # to calculate the accuracy. Code below\n",
    "# )\n",
    "\n",
    "# trainer.train() for training. Will not show how well the model is doing. To add that, add the compute metrics and change from train() to evaluate()\n",
    "# save the model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# method used to calculate accuracy\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Use the trained model to predict output of the test dataset and calculate the accuracy\n",
    "#TODO load saved model\n",
    "#TODO define the trainer\n",
    "#Example testing_trainer = Trainer(model)\n",
    "\n",
    "#TODO Make the prediction using the trainer. the result is logit \n",
    "\n",
    "#TODO preprocess the prediction to get the labels\n",
    "\n",
    "#TODO calcuate the accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO look for a new sentence and predict to see how the model is performing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO -train using native code\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "# model.to(device)\n",
    "# model.train()\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# for epoch in range(3):\n",
    "#     for batch in train_loader:\n",
    "#         optim.zero_grad()\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs[0]\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "\n",
    "# model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}