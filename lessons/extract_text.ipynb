{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install texttract,tweepy,bs4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import textract\n",
    "import tweepy\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir(\"/content/gdrive/My Drive/nlp_training_ff_giz\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read files\n",
    "This function will be used to read files. It will accept a filepath and the type and return a data frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def readFile(filepath,filetype):\n",
    "    # filepath is the path to the file you want to read\n",
    "    # filetype is the file format. it can be excel,csv,json,sas,xml,html\n",
    "    # text is the resulting dataframe - columns (index,sentence)\n",
    "    text = None\n",
    "    if filetype == 'csv':\n",
    "        text = pd.read_csv(filepath)\n",
    "    elif filetype == 'excel':\n",
    "        #TODO Exercise -read excel file\n",
    "        text = None\n",
    "    elif filetype == 'json':\n",
    "        #TODO Exercise - read json file\n",
    "        text = None\n",
    "    elif filetype == 'txt':\n",
    "        lines = open(filepath,\"r\")\n",
    "        data = []\n",
    "        for l in lines:\n",
    "            if len(l.strip()) != 0:\n",
    "                data.append(l.strip())\n",
    "        #TODO Excerise - create a dataframe with the data\n",
    "        text = None\n",
    "    elif filetype == 'pdf':\n",
    "        lines = textract.process(filepath, extension='pdf', method='tesseract')\n",
    "        lines = lines.decode(\"utf-8\")\n",
    "        data = []\n",
    "        #TODO Excerise - create a dataframe with the data\n",
    "        for x in lines.split('\\n'):\n",
    "            if x != '':\n",
    "                data.append(x)\n",
    "        text = None\n",
    "    elif filetype == 'docx':\n",
    "        #TODO Excercise - read docx (use textract)\n",
    "        text = None\n",
    "\n",
    "\n",
    "\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scrap a website article\n",
    "This function will be used to extract text from a website. The function will accept a link and return the text extracted as a list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrapwebsite(link):\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text,'html') #get the whole page. can either be lxml or html depending on the side\n",
    "    main = soup.find('main') # get the main component where your data is \n",
    "    para = main.find_all('p') # look for the recurring element that in the main component\n",
    "    text = []\n",
    "    \n",
    "    cols = [ele.text.strip() for ele in para] # loop through the component and extract the text\n",
    "    for x in cols:\n",
    "        if len(x) > 0: # remove the empty spaces\n",
    "            \n",
    "            text.append(x)\n",
    "    \n",
    "    return text\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text  = scrapwebsite(\"https://www.bbc.com/gahuza/amakuru-57846310\")\n",
    "text_df = pd.DataFrame(text, columns=['sentence'])\n",
    "text_df.to_csv('../data/extract_data/bbc.csv')\n",
    "#TODO Exercise - get 20 articles from igihe\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scrap Twitter\n",
    "To get tweets, you will need a developer account. https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api\n",
    "You will create an App and get Keys"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrapeTwitter():\n",
    "    consumer_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" \n",
    "    consumer_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" \n",
    "    access_token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" \n",
    "    access_token_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    tweet_lst=[]\n",
    "    for tweet in tweepy.Cursor(api.search, q='rbcrwanda').items(1000):\n",
    "        \n",
    "        tweetDate = tweet.created_at.date()\n",
    "        tweet_lst.append([tweetDate,tweet.id,                    \n",
    "                   tweet.text])\n",
    "    tweet_df = pd.DataFrame(tweet_lst, columns=['tweet_dt', 'id', 'tweet'])\n",
    "    tweet_df.to_csv('../data/extract_data/rbc_tweets.csv')\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scrapeTwitter()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}