{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python365jvsc74a57bd0bc523c9c715ae469085a21d88ef5ef90b659fdfad3965f9fb06796bcbb51bb0e",
   "display_name": "Python 3.6.5 64-bit ('amazonei_tensorflow_p36': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "bc523c9c715ae469085a21d88ef5ef90b659fdfad3965f9fb06796bcbb51bb0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, TimeDistributed\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Bidirectional, Concatenate, Lambda\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "snapshot_folder = './chatbot_weights_v2'\n",
    "data_path = './movie_dialogue/'\n",
    "\n",
    "# assert os.path.isdir(snapshot_folder) == True\n",
    "# assert os.path.isdir(data_path) == True\n",
    "print(tf.__version__)\n",
    "\n",
    "test = False\n",
    "print('test: ', test)\n",
    "if test:\n",
    "    GRU_units = 10\n",
    "    batch_size = 4\n",
    "    emb_dim = 10\n",
    "else:\n",
    "    GRU_units = 256\n",
    "    batch_size = 32\n",
    "    emb_dim = 50\n",
    "\n",
    "init_lr = 0.0005"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def progressBar(value, endvalue, bar_length=20, job='Job'):\n",
    "\n",
    "    percent = float(value) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    sys.stdout.write(\"\\r{0} Completion: [{1}] {2}%\".format(job,arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def print_tensor(t):\n",
    "    print(K.get_value(t))\n",
    "    \n",
    "def to_tensor(t):\n",
    "    return tf.convert_to_tensor(t)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the data\n",
    "lines = open(data_path+'movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open(data_path+'movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n') # index of related lines\n",
    "\n",
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "        \n",
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = [ ]\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))\n",
    "\n",
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "pairs = []\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        pairs.append([id2line[conv[i]],id2line[conv[i+1]]])\n",
    "        \n",
    "# Check if we have loaded the data correctly\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(pairs[i][0])\n",
    "    print(pairs[i][1])\n",
    "    print()\n",
    "    \n",
    "len(pairs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# replacing many abbreviations and lower casing the words\n",
    "def replace_phrase(pairs):\n",
    "    p = pairs.copy()\n",
    "\n",
    "    for i in p:\n",
    "        for j in range(0,2):\n",
    "            i[j] = i[j].lower()\n",
    "            i[j] = re.sub(r\"there's\", \"there is\", i[j])\n",
    "            i[j] = re.sub(r\"i'm\", \"i am\", i[j])\n",
    "            i[j] = re.sub(r\"he's\", \"he is\", i[j])\n",
    "            i[j] = re.sub(r\"she's\", \"she is\", i[j])\n",
    "            i[j] = re.sub(r\"it's\", \"it is\", i[j])\n",
    "            i[j] = re.sub(r\"that's\", \"that is\", i[j])\n",
    "            i[j] = re.sub(r\"what's\", \"that is\", i[j])\n",
    "            i[j] = re.sub(r\"where's\", \"where is\", i[j])\n",
    "            i[j] = re.sub(r\"how's\", \"how is\", i[j])\n",
    "            i[j] = re.sub(r\"\\'ll\", \" will\", i[j])\n",
    "            i[j] = re.sub(r\"\\'ve\", \" have\", i[j])\n",
    "            i[j] = re.sub(r\"\\'re\", \" are\", i[j])\n",
    "            i[j] = re.sub(r\"\\'d\", \" would\", i[j])\n",
    "            i[j] = re.sub(r\"\\'re\", \" are\", i[j])\n",
    "            i[j] = re.sub(r\"won't\", \"will not\", i[j])\n",
    "            i[j] = re.sub(r\"can't\", \"cannot\", i[j])\n",
    "            i[j] = re.sub(r\"n't\", \" not\", i[j])\n",
    "            i[j] = re.sub(r\"n'\", \"ng\", i[j])\n",
    "            i[j] = re.sub(r\"'bout\", \"about\", i[j])\n",
    "            i[j] = re.sub(r\"'til\", \"until\", i[j])\n",
    "            i[j] = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", i[j])\n",
    "            i[j] = i[j].strip()\n",
    "    return p\n",
    "replaced_pairs = replace_phrase(pairs)\n",
    "replaced_pairs[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_data(pairs):\n",
    "    p = pairs.copy()\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for i in p:\n",
    "        # tokenize\n",
    "        i[0], i[1] = i[0].split(), i[1].split()\n",
    "        # convert to lower case\n",
    "        i[0], i[1] = [word.lower() for word in i[0]], [word.lower() for word in i[1]]\n",
    "        # remove punctuation from each token\n",
    "        i[0], i[1] = [w.translate(table) for w in i[0]], [w.translate(table) for w in i[1]]\n",
    "        # remove tokens with numbers in them\n",
    "        i[0], i[1] = [word for word in i[0] if word.isalpha()], [word for word in i[1] if word.isalpha()]\n",
    "        # store as string\n",
    "        i[0], i[1] =  ' '.join(i[0]), ' '.join(i[1])\n",
    "            \n",
    "    return p\n",
    "\n",
    "clean_pairs = clean_data(replaced_pairs)\n",
    "clean_pairs[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# adding the start and end token to our sentences\n",
    "start_token = '<startseq>'\n",
    "end_token = '<endseq>'\n",
    "\n",
    "def add_end_start_tokens(pairs):\n",
    "    p = pairs.copy()\n",
    "    for i in p:\n",
    "        i[0] = start_token + ' '  + i[0] + ' ' + end_token\n",
    "        i[1] = start_token + ' '  + i[1] + ' ' + end_token\n",
    "    return p\n",
    "\n",
    "tokenized_pairs = add_end_start_tokens(clean_pairs)\n",
    "tokenized_pairs[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding the maximum length of questions and answers\n",
    "# because there are senteces with unusually long lengths, we caculate the max length that 80% of data can be placed in\n",
    "def max_length(pairs,prct):\n",
    "    # Create a list of all the captions\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for i in pairs:\n",
    "        questions.append(i[0])\n",
    "        answers.append(i[1])\n",
    "        \n",
    "    length_questions = list(len(d.split()) for d in questions)\n",
    "    length_answers = list(len(d.split()) for d in answers)\n",
    "\n",
    "    print('percentile {} of len of questions: {}'.format(prct,np.percentile(length_questions, prct)))\n",
    "    print('longest sentence: ', max(length_questions))\n",
    "    print()\n",
    "    print('percentile {} of len of answers: {}'.format(prct,np.percentile(length_answers, prct)))\n",
    "    print('longest sentence: ', max(length_answers))\n",
    "    print()\n",
    "    return int(np.percentile(length_questions, prct)),int(np.percentile(length_answers, prct))\n",
    "\n",
    "max_len_q,max_len_a = max_length(tokenized_pairs,80)\n",
    "\n",
    "print('max-len questions for training: ', max_len_q)\n",
    "print('max-len answers for training: ', max_len_a)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove questions and answers that are shorter than 2 words and longer than maxlen.\n",
    "min_line_len = 2 # two words are for tokens\n",
    "\n",
    "def set_length(tokenized_pairs):\n",
    "    pairs_final = []\n",
    "    for p in tokenized_pairs:\n",
    "        if (\n",
    "            len(p[0].split())>=min_line_len and len(p[1].split())>=min_line_len \n",
    "           and len(p[0].split())<=max_len_q and len(p[1].split())<=max_len_a):\n",
    "                \n",
    "            pairs_final.append(p)\n",
    "            \n",
    "    return pairs_final\n",
    "\n",
    "pairs_final = set_length(tokenized_pairs)\n",
    "len(pairs_final)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# making a vocabulary of the words that occur more than word_count_threshold time\n",
    "def create_reoccurring_vocab(pairs, word_count_threshold = 5):\n",
    "    p = pairs\n",
    "    # Create a list of all the captions\n",
    "    all_captions = []\n",
    "    for i in p:\n",
    "        for j in i:\n",
    "            all_captions.append(j)\n",
    "\n",
    "    # Consider only words which occur at least 10 times in the corpus\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in all_captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    vocab = list(set(vocab))\n",
    "    print('Short vocab size: %d ' % len(vocab))\n",
    "    return vocab\n",
    "\n",
    "# each word in the vocabulary must be used in the data atleast 20 times\n",
    "short_vocab = create_reoccurring_vocab(pairs_final, word_count_threshold = 4)\n",
    "# removing one character words from vocab except for 'a'\n",
    "for v in short_vocab:\n",
    "    if len(v) == 1 and v!='a' and v!='i':\n",
    "        short_vocab.remove(v) \n",
    "\n",
    "short_vocab = sorted(short_vocab)[1:]\n",
    "short_vocab[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab_len = len(short_vocab) + 1 # since index 0 is used as padding, we have to increase the vocab size\n",
    "vocab_len"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# keep the pairs that have the words in vocab\n",
    "def trimRareWords(voc, pairs):\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    i=0\n",
    "    for pair in pairs:\n",
    "        i+=1\n",
    "        progressBar(value=i,endvalue=len(pairs))\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"\\nTrimmed from {} pairs to {}\".format(len(pairs), len(keep_pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# # Trim voc and pairs\n",
    "pairs_final = trimRareWords(short_vocab, pairs_final)\n",
    "with open ('final_pairs_v21.pkl','wb') as f:\n",
    "    pairs_final = pickle.dump(pairs_final,f)\n",
    "    \n",
    "with open ('final_pairs_v21.pkl','rb') as f:\n",
    "    pairs_final = pickle.load(f)\n",
    "    \n",
    "pairs_final_train = pairs_final\n",
    "len(pairs_final_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create an instance of the tokenizer object:\n",
    "tokenizer = Tokenizer(filters = [])\n",
    "tokenizer.fit_on_texts(short_vocab)\n",
    "\n",
    "ixtoword = {} # index to word dic\n",
    "wordtoix = tokenizer.word_index # word to index dic\n",
    "pad_token = 'pad0'\n",
    "ixtoword[0] = pad_token # no word in vocab has index 0. but padding is indicated with 0\n",
    "\n",
    "for w in tokenizer.word_index:\n",
    "    ixtoword[tokenizer.word_index[w]] = w\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# making the model\n",
    "\n",
    "# Making the embedding mtrix\n",
    "def make_embedding_layer(embedding_dim=100, glove=True):\n",
    "    if glove == False:\n",
    "        print('Just a zero matrix loaded')\n",
    "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # just a zero matrix \n",
    "    else:\n",
    "        print('Loading glove...')\n",
    "        glove_dir = '../../../glove'\n",
    "        embeddings_index = {} \n",
    "        f = open(os.path.join(glove_dir, 'glove.6B.'+str(embedding_dim)+'d.txt'), encoding=\"utf-8\")\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        print(\"GloVe \",embedding_dim, ' loded!')\n",
    "        # Get 200-dim dense vector for each of the vocab_rocc\n",
    "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
    "        for word, i in wordtoix.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # Words not found in the embedding index will be all zeros\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) # we have a limited vocab so we \n",
    "                                                                                           # do not train the embedding layer\n",
    "                                                                                           # we use 0 as padding so => mask_zero=True\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([embedding_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "embeddings = make_embedding_layer(embedding_dim=emb_dim, glove=not test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.Bidirectional1 = Bidirectional(GRU(enc_units, return_sequences=True,\n",
    "                               return_state=False, recurrent_initializer='glorot_uniform', name='gru_1'), name='bidirectional_encoder1')\n",
    "        self.Bidirectional2 = Bidirectional(GRU(enc_units, return_sequences=True, \n",
    "                               return_state=True, recurrent_initializer='glorot_uniform', name='gru_2'), name='bidirectional_encoder2')\n",
    "                                                                                                \n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.Inp = Input(shape=(max_len_q,)) # size of questions\n",
    "            \n",
    "    def bidirectional(self, bidir, layer, inp, hidden):\n",
    "        return bidir(layer(inp, initial_state = hidden))\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.Bidirectional1(x)\n",
    "        x = self.dropout(x)\n",
    "        output, state_f,state_b = self.Bidirectional2(x)\n",
    "\n",
    "        return output, state_f, state_b\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder = Encoder(vocab_len, emb_dim, GRU_units)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.units = units\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        \n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_size\n",
    "        self.embeddings = embeddings\n",
    "        self.units = 2 * dec_units # because we use bidirectional encoder\n",
    "        self.fc = Dense(vocab_len, activation='softmax', name='dense_layer')\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        self.decoder_gru_l1 = GRU(self.units, return_sequences=True, \n",
    "                                  return_state= False, recurrent_initializer='glorot_uniform' ,name='decoder_gru1')\n",
    "        self.decoder_gru_l2 = GRU(self.units, return_sequences=False, \n",
    "                                  return_state= True, recurrent_initializer='glorot_uniform' ,name='decoder_gru2') \n",
    "        self.dropout = Dropout(0.4)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # concat input and context vector together\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        x = self.decoder_gru_l1(x)\n",
    "        x = self.dropout(x)\n",
    "        output, state = self.decoder_gru_l2(x)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import unicodedata\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(sentence):\n",
    "    \n",
    "    attention_plot = np.zeros((max_len_a, max_len_q))\n",
    "\n",
    "    sentence = unicode_to_ascii(sentence.lower())\n",
    "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
    "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
    "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, GRU_units))]\n",
    "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "    dec_input = tf.expand_dims([wordtoix[start_token]], 1)\n",
    "\n",
    "    for t in range(max_len_a):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = K.get_value(attention_weights)\n",
    "        \n",
    "        predicted_id =  K.get_value(tf.argmax(predictions[0]))       \n",
    "\n",
    "        if ixtoword[predicted_id] == end_token:\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        result += ixtoword[predicted_id] + ' '\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def answer(sentence, training=False):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    if training:\n",
    "        return result\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted answer: {}'.format(result))\n",
    "    attention_plot = attention_plot[1:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' ')[:-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def beam_search(sentence, k=3, maxsample=max_len_a, use_unk=False, oov=None, eos=end_token):\n",
    "    \"\"\"\n",
    "        return k samples and NLL scores\n",
    "    \"\"\"\n",
    "    \n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [[wordtoix[start_token]]]\n",
    "    live_scores = [0]\n",
    "\n",
    "    sentence = unicode_to_ascii(sentence.lower())\n",
    "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
    "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
    "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    hidden = [tf.zeros((1, GRU_units))]\n",
    "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "    dec_input = tf.expand_dims([wordtoix[start_token]], 0)\n",
    "        \n",
    "    while live_k and dead_k < k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        predictions, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
    "        probs = K.get_value(predictions[0])\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        cand_flat = cand_scores.flatten()\n",
    "\n",
    "        # find the best (lowest) scores we have from all possible samples and new words\n",
    "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "        live_scores = cand_flat[ranks_flat]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = vocab_len\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    final_samples = dead_samples + live_samples\n",
    "    final_scores = dead_scores + live_scores   \n",
    "    \n",
    "    # cutting the strong where end_token is encounterd\n",
    "    for i in range(len(final_scores)):\n",
    "        final_scores[i] /= len(final_samples[i]) # normalizing the scores\n",
    "    \n",
    "    final_result =[]\n",
    "    \n",
    "    for i in range(len(final_scores)):\n",
    "        final_result.append((final_scores[i],final_samples[i]))\n",
    "    \n",
    "    final_list_ix = max(final_result)[1]\n",
    "    final_list_word = [ixtoword[f] for f in final_list_ix]\n",
    "    final_sentence = ' '.join(final_list_word[1:])\n",
    "    end_ix = final_sentence.find(end_token)\n",
    "    return final_sentence[:end_ix]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = tf.keras.optimizers.Adam(init_lr)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = K.sparse_categorical_crossentropy(real, pred, from_logits= False)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(snapshot_folder, str(emb_dim)+\"-ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden_f, enc_hidden_b = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "        dec_input = tf.expand_dims([wordtoix[start_token]] * batch_size, 1) # dec_input initially == start_token\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            \n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions) # each time just use one timestep output\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1) # expected output at this time becomes input for next timestep\n",
    "            \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history={'loss':[]}\n",
    "smallest_loss = np.inf\n",
    "best_ep = 1\n",
    "EPOCHS = 1000 # but 150 is enough\n",
    "enc_hidden = encoder.initialize_hidden_state()\n",
    "steps_per_epoch = len(pairs_final_train)//batch_size # used for caculating number of batches\n",
    "current_ep = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_bot(k = 3, beam = False):\n",
    "    print('#'*20)\n",
    "    q = 'Hello'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'How are you'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q= 'Are you my friend'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'What are you doing'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'What your favorite restaurant'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'Who are you'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'Do you want to go out'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('#'*20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_history():\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.plot(best_ep,smallest_loss,'ro')\n",
    "    plt.plot(history['loss'],'b-')\n",
    "    plt.legend(['best','loss'])\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_loss = K.constant(0)\n",
    "X, y = [], []\n",
    "for ep in range(current_ep,EPOCHS):\n",
    "    current_ep = ep    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    btch = 1\n",
    "\n",
    "    for p in pairs_final_train:     \n",
    "        \n",
    "        question = p[0]\n",
    "        label = p[1]\n",
    "        # find the index of each word of the caption in vocabulary\n",
    "        question_seq = [wordtoix[word] for word in question.split(' ') if word in wordtoix]\n",
    "        label_seq = [wordtoix[word] for word in label.split(' ') if word in wordtoix]\n",
    "        # encoder input and decoder input and label\n",
    "        enc_in_seq = pad_sequences([question_seq], maxlen=max_len_q, padding='post')[0]\n",
    "        dec_out_seq = pad_sequences([label_seq], maxlen=max_len_a, padding='post')[0]\n",
    "        \n",
    "        X.append(enc_in_seq)\n",
    "        y.append(dec_out_seq)\n",
    "\n",
    "        if len(X) == batch_size :\n",
    "            batch_loss = train_step(np.array(X), np.array(y), enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "            X , y = [], []\n",
    "            btch += 1\n",
    "            if btch % (steps_per_epoch//6) == 0:\n",
    "                print('Epoch {} Batch {} Loss: {:.4f}'.format(ep , btch, K.get_value(batch_loss)))\n",
    "\n",
    "    epoch_loss =  K.get_value(total_loss) / steps_per_epoch\n",
    "    print('\\n*** Epoch {} Loss {:.4f} ***\\n'.format(ep ,epoch_loss))\n",
    "    history['loss'].append(epoch_loss)\n",
    "    \n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    test_bot(k=5)\n",
    "\n",
    "    if epoch_loss < smallest_loss:\n",
    "        smallest_loss = epoch_loss\n",
    "        best_ep = ep \n",
    "        print('check point saved!')\n",
    "    \n",
    "    if ep % 3 == 0:\n",
    "        plot_history()\n",
    "        \n",
    "    print('Best epoch so far: ',best_ep,' smallest loss:',smallest_loss)\n",
    "    print('Time taken for the epoch {:.3f} sec\\n'.format(time.time() - start))\n",
    "\n",
    "    print('=' * 40)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}